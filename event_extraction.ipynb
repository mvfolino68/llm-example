{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvfolino68/llm-example/blob/main/event_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0XcfvIuRxmF"
      },
      "source": [
        "# ðŸ“… LLM Event Extraction Workshop\n",
        "\n",
        "This notebook demonstrates how to use Large Language Models (specifically OpenAI's `gpt-4o`) to extract calendar event details from everyday conversational text. We'll walk through building a practical tool that could be integrated into email assistants, chat applications, or productivity tools.\n",
        "\n",
        "## ðŸ‘‹ Workshop Goals\n",
        "* Understand prompt chaining for multi-step LLM workflows\n",
        "* Learn how to use Pydantic for structured LLM outputs\n",
        "* Build a complete, production-ready event extraction system\n",
        "* Explore how to make LLMs work as reliable components in larger applications\n",
        "\n",
        "**Presented:** March 08, 2025\n",
        "\n",
        "## ðŸ” What is Prompt Chaining?\n",
        "\n",
        "**Prompt chaining** decomposes a complex task into a sequence of simpler steps, where each LLM call processes the output of the previous one. This approach offers several advantages:\n",
        "\n",
        "* **Improved accuracy** - Each step has a clearer, more focused goal\n",
        "* **Better control** - You can add validation between steps\n",
        "* **Easier debugging** - When issues occur, you can identify exactly which step failed\n",
        "\n",
        "In our workflow today:\n",
        "1. First, we determine IF text contains a calendar event\n",
        "2. Then, we extract the detailed event information\n",
        "3. Finally, we generate a natural-language confirmation\n",
        "\n",
        "Let's get started by setting up our environment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Setup and Installation\n",
        "\n",
        "First, we need to install our required packages and set up our API access. For this workshop, we'll need:\n",
        "* `openai` - To access OpenAI's API\n",
        "* `pydantic` - For type validation and structured outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install openai==1.20.0 pydantic==2.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLmUBICXRxmG"
      },
      "outputs": [],
      "source": [
        "# Setup: Import libraries and initialize client\n",
        "from typing import Optional\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# For this workshop, you'll need your own OpenAI API key\n",
        "# Option 1: Enter it directly when prompted\n",
        "try:\n",
        "    # Try to get from Colab secrets first (if you've set it up)\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"Using API key from Colab secrets\")\n",
        "except:\n",
        "    # Fallback to manual input\n",
        "    api_key = input(\"Enter your OpenAI API key: \")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "    print(\"Using manually entered API key\")\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(api_key=api_key)\n",
        "model = \"gpt-4o\"\n",
        "print(\"âœ… Client initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Data Models with Pydantic\n",
        "\n",
        "A key concept in building reliable AI applications is **structured outputs**. Instead of parsing free-form text from the LLM, we can have it generate data in precise formats.\n",
        "\n",
        "**Pydantic** is a Python library that helps us define data models with type validation. When combined with OpenAI's structured output feature, it ensures the LLM generates responses that exactly match our expected schema.\n",
        "\n",
        "Let's define three models for our event extraction workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpbsL45KRxmG"
      },
      "outputs": [],
      "source": [
        "# Step 1: Define data models\n",
        "class EventExtraction(BaseModel):\n",
        "    description: str            # Cleaned version of the input text\n",
        "    is_calendar_event: bool     # Binary classification: is this an event?\n",
        "    confidence_score: float     # How confident is the model (0.0-1.0)\n",
        "\n",
        "class EventDetails(BaseModel):\n",
        "    name: str                   # Event title/name \n",
        "    date: str                   # ISO 8601 formatted date with time\n",
        "    duration_minutes: int       # How long the event lasts\n",
        "    participants: list[str]     # Who is attending\n",
        "\n",
        "class EventConfirmation(BaseModel):\n",
        "    confirmation_message: str             # Human-friendly confirmation\n",
        "    calendar_link: Optional[str] = None   # Optional calendar link\n",
        "\n",
        "print(\"âœ… Data models defined - these will ensure our LLM outputs follow a consistent format.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ•µï¸ Step 1: Event Detection\n",
        "\n",
        "The first step in our prompt chain is to determine whether a given text contains a calendar event. This acts as a \"filter\" to avoid wasting compute time on non-event texts.\n",
        "\n",
        "Note how we use the `parse` method with our `EventExtraction` model to get structured output rather than free text. This is a powerful technique introduced in the OpenAI API that ensures data consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ3ICYmFRxmG"
      },
      "outputs": [],
      "source": [
        "# Step 2: Extract event info - The first link in our prompt chain\n",
        "def extract_event_info(user_input: str) -> EventExtraction:\n",
        "    # Include current date for context (helps with relative dates like \"next Tuesday\")\n",
        "    today = datetime.now().strftime(\"%A, %B %d, %Y\")\n",
        "    \n",
        "    # Call the OpenAI API using our structured format\n",
        "    completion = client.beta.chat.completions.parse(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": f\"Today is {today}. Analyze the user message and determine if it contains a calendar event request. Extract relevant details and provide a confidence score between 0 and 1.\"},\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ],\n",
        "        response_format=EventExtraction,  # This tells the API to format output as our model\n",
        "    )\n",
        "    return completion.choices[0].message.parsed\n",
        "\n",
        "# Test with a sample input\n",
        "input_text = \"Let's schedule a 1h team meeting next Tuesday at 2pm with Alice and Bob.\"\n",
        "result = extract_event_info(input_text)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ Step 2: Detail Extraction\n",
        "\n",
        "Now that we've confirmed the text contains a calendar event, we'll extract specific details like the event name, date, duration, and participants.\n",
        "\n",
        "This is the second link in our prompt chain - it takes the description from the previous step and extracts structured information. By breaking this out as a separate step, we give the model a more focused task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0uRPENzRxmG"
      },
      "outputs": [],
      "source": [
        "# Step 3: Parse event details - The second link in our prompt chain\n",
        "def parse_event_details(description: str) -> EventDetails:\n",
        "    today = datetime.now().strftime(\"%A, %B %d, %Y\")\n",
        "    completion = client.beta.chat.completions.parse(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": f\"Today is {today}. Extract specific event details from the text. Format the date in ISO 8601 format (YYYY-MM-DDTHH:MM:SS). If multiple people are mentioned, include all in the participants list.\"},\n",
        "            {\"role\": \"user\", \"content\": description},\n",
        "        ],\n",
        "        response_format=EventDetails,\n",
        "    )\n",
        "    return completion.choices[0].message.parsed\n",
        "\n",
        "# Use previous output as input to this function\n",
        "details = parse_event_details(result.description)\n",
        "details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¨ Step 3: Confirmation Generation\n",
        "\n",
        "The final step in our chain is to generate a natural-language confirmation message. This demonstrates how we can convert structured data back into human-friendly text.\n",
        "\n",
        "This approach is powerful because:\n",
        "1. We maintain structured data throughout our workflow (for database storage, API responses, etc.)\n",
        "2. We can still provide a conversational, personalized experience to users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kSpuZXJRxmG"
      },
      "outputs": [],
      "source": [
        "# Step 4: Generate confirmation - The third link in our prompt chain\n",
        "def generate_confirmation(event_details: EventDetails) -> EventConfirmation:\n",
        "    completion = client.beta.chat.completions.parse(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful personal assistant named Susie. Generate a friendly, concise confirmation message based on the event details provided. Include all important information in a natural way. Sign off with 'Susie'.\"},\n",
        "            {\"role\": \"user\", \"content\": str(event_details.model_dump())},\n",
        "        ],\n",
        "        response_format=EventConfirmation,\n",
        "    )\n",
        "    return completion.choices[0].message.parsed\n",
        "\n",
        "# Use previous output as input to this function\n",
        "confirmation = generate_confirmation(details)\n",
        "confirmation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”„ Complete Workflow\n",
        "\n",
        "Now let's connect all three steps into a single workflow. This demonstrates the complete prompt chain:\n",
        "\n",
        "1. **Extract** - Determine if text contains an event\n",
        "2. **Parse** - Extract structured details from the text\n",
        "3. **Generate** - Create a human-friendly confirmation\n",
        "\n",
        "Notice how we include a validation step after the first function call. This is a \"gate\" that prevents low-confidence or non-event inputs from proceeding, saving compute and improving reliability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ckngvl-ARxmG"
      },
      "outputs": [],
      "source": [
        "# Step 5: Full workflow - The complete prompt chain\n",
        "def process_calendar_request(user_input: str) -> Optional[EventConfirmation]:\n",
        "    # Step 1: Check if input contains a calendar event\n",
        "    extraction = extract_event_info(user_input)\n",
        "    \n",
        "    # Validation gate: Only proceed if we're confident this is a calendar event\n",
        "    if not extraction.is_calendar_event or extraction.confidence_score < 0.7:\n",
        "        print(\"Not a calendar event or low confidence. Stopping workflow.\")\n",
        "        return None\n",
        "    \n",
        "    # Step 2: Extract detailed information\n",
        "    details = parse_event_details(extraction.description)\n",
        "    \n",
        "    # Step 3: Generate user-friendly confirmation\n",
        "    return generate_confirmation(details)\n",
        "\n",
        "# Test the full workflow with our example\n",
        "result = process_calendar_request(input_text)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§ª Try Your Own Examples!\n",
        "\n",
        "Now it's your turn to experiment! Try different inputs to see how the system handles various phrasings, edge cases, and non-event texts.\n",
        "\n",
        "### Workshop Challenges:\n",
        "\n",
        "1. **Basic**: Try different ways of phrasing calendar events\n",
        "2. **Intermediate**: Test with ambiguous dates or unusual time formats\n",
        "3. **Advanced**: Try inputs that mix event details with other content\n",
        "4. **Expert**: Modify the models to include additional fields (location, priority, etc.)\n",
        "\n",
        "Remember that the quality of the input prompt greatly affects the output. This is a great opportunity to practice prompt engineering!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your turn! Try different inputs to see how the system responds\n",
        "your_text = \"\"\"\n",
        "I'd like to meet for coffee sometime next week to discuss the project proposal.\n",
        "How about Wednesday afternoon around 3pm? We should invite the marketing team too.\n",
        "\"\"\"\n",
        "\n",
        "# Process your custom text\n",
        "your_result = process_calendar_request(your_text)\n",
        "your_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ Extending The Application\n",
        "\n",
        "### Ideas for Further Development:\n",
        "\n",
        "1. **Add location extraction**: Enhance the `EventDetails` model to include location information\n",
        "2. **Implement recurring events**: Add fields for frequency and pattern detection\n",
        "3. **Connect to calendar APIs**: Add integration with Google Calendar or Outlook\n",
        "4. **Add conflict detection**: Check against existing calendar events\n",
        "5. **Implement multi-language support**: Test and tune prompts for different languages\n",
        "\n",
        "### Prompt Chaining Best Practices:\n",
        "\n",
        "* **Keep each step focused**: Each prompt should have a single, clear purpose\n",
        "* **Add validation between steps**: Check outputs before proceeding to the next step\n",
        "* **Use structured outputs**: Define clear schemas for each step's output\n",
        "* **Include contextual information**: Date, time, user preferences, etc.\n",
        "* **Design for error handling**: What happens when a step fails?\n",
        "\n",
        "### When to Use Prompt Chaining:\n",
        "\n",
        "Prompt chaining is ideal when:\n",
        "* Tasks can be decomposed into logical subtasks\n",
        "* You need high precision and reliability\n",
        "* You want to implement validation gates\n",
        "* You're building complex, multi-step workflows\n",
        "\n",
        "## ðŸ™ Thank You!\n",
        "\n",
        "We hope you enjoyed this workshop on building practical AI applications with LLMs and prompt chaining. Feel free to adapt this code for your own projects!\n",
        "\n",
        "**Resources:**\n",
        "* [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)\n",
        "* [Pydantic Documentation](https://docs.pydantic.dev/)\n",
        "* [Prompt Engineering Guide](https://www.promptingguide.ai/)"
      ]
    }
  ]
}
