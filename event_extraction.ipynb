{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvfolino68/llm-example/blob/main/event_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“… LLM Event Extraction Workshop - AI Systems\n",
        "\n",
        "This notebook demonstrates how to use Large Language Models (specifically OpenAI's `gpt-4o-mini`) to extract calendar event details from everyday conversational text. We'll walk through building a practical tool that could be integrated into email assistants, chat applications, or productivity tools.\n",
        "\n",
        "## ðŸ‘‹ Goals\n",
        "* Understand prompt chaining for multi-step LLM workflows\n",
        "* Learn how to use Pydantic for structured LLM outputs\n",
        "* Build an event extraction system\n",
        "* Explore how to make LLMs work as reliable components in larger applications\n"
      ],
      "metadata": {
        "id": "hPx6wQpKY8D1"
      },
      "id": "hPx6wQpKY8D1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ” What is Prompt Chaining?\n",
        "\n",
        "**Prompt chaining** decomposes a complex task into a sequence of simpler steps, where each LLM call processes the output of the previous one. This approach offers several advantages:\n",
        "\n",
        "* **Improved accuracy** - Each step has a clearer, more focused goal\n",
        "* **Better control** - You can add validation between steps\n",
        "* **Easier debugging** - When issues occur, you can identify exactly which step failed\n",
        "\n",
        "In our workflow today:\n",
        "1. First, we determine IF text contains a calendar event\n",
        "2. Then, we extract the detailed event information\n",
        "3. Finally, we generate a natural-language confirmation\n",
        "\n",
        "Let's get started by setting up our environment!\n"
      ],
      "metadata": {
        "id": "CyWJG6vAZzS9"
      },
      "id": "CyWJG6vAZzS9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”§ Setup and Installation\n",
        "Add OpenAI secret to Colab Secrets on the left. ðŸ”‘\n",
        "\n",
        "Name the secret `OPENAI_API_KEY` and make it avilable to the notebook.\n",
        "\n",
        "We'll share a 1password link with openai api key."
      ],
      "metadata": {
        "id": "3W8pkgGzbQLm"
      },
      "id": "3W8pkgGzbQLm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-2AV_D-YMT5"
      },
      "outputs": [],
      "source": [
        "# Setup: Import libraries and initialize client\n",
        "from typing import Optional\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "\n",
        "# For this workshop, you'll need your own OpenAI API key.\n",
        "# Enter it in notebook secrets to the left. Name the secret `OPENAI_API_KEY`\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(api_key=api_key)\n",
        "model = \"gpt-4o-mini\"\n",
        "print(\"âœ… Client initialized successfully!\")\n"
      ],
      "id": "H-2AV_D-YMT5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Š Step 1: Data Models with Pydantic\n",
        "\n",
        "A key concept in building reliable AI applications is **structured outputs**. Instead of parsing free-form text from the LLM, we can have it generate data in precise formats.\n",
        "\n",
        "**Pydantic** helps us define data models with type validation. When combined with OpenAI's structured output feature, it ensures the LLM generates responses that exactly match our expected schema.\n",
        "\n",
        "Let's define three models for our event extraction workflow:\n"
      ],
      "metadata": {
        "id": "WkkyiJfycMJK"
      },
      "id": "WkkyiJfycMJK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX4XClskYMT5"
      },
      "outputs": [],
      "source": [
        "# Step 1: Define data models\n",
        "class EventExtraction(BaseModel):\n",
        "    description: str            # Cleaned version of the input text\n",
        "    is_calendar_event: bool     # Binary classification: is this an event?\n",
        "    confidence_score: float     # How confident is the model (0.0-1.0)\n",
        "\n",
        "class EventDetails(BaseModel):\n",
        "    name: str                   # Event title/name\n",
        "    date: str                   # ISO 8601 formatted date with time\n",
        "    duration_minutes: int       # How long the event lasts\n",
        "    participants: list[str]     # Who is attending\n",
        "\n",
        "class EventConfirmation(BaseModel):\n",
        "    confirmation_message: str             # Human-friendly confirmation\n",
        "    calendar_link: Optional[str] = None   # Optional calendar link\n",
        "\n",
        "print(\"âœ… Data models defined - these will ensure our LLM outputs follow a consistent format.\")\n"
      ],
      "id": "vX4XClskYMT5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ•µï¸ Step 2: Event Detection\n",
        "\n",
        "The first step in our prompt chain is to determine whether a given text contains a calendar event. This acts as a \"filter\" to avoid wasting compute time on non-event texts.\n",
        "\n",
        "Note how we use the `parse` method with our `EventExtraction` model to get structured output rather than free text. This is a powerful technique introduced in the OpenAI API that ensures data consistency.\n"
      ],
      "metadata": {
        "id": "K5NRQrtjcZHP"
      },
      "id": "K5NRQrtjcZHP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AP_kX8SYMT5"
      },
      "outputs": [],
      "source": [
        "# Step 2: Extract event info - The first link in our prompt chain\n",
        "def extract_event_info(user_input: str) -> EventExtraction:\n",
        "    # Include current date for context (helps with relative dates like \"next Tuesday\")\n",
        "    today = datetime.now().strftime(\"%A, %B %d, %Y\")\n",
        "\n",
        "    # Call the OpenAI API using our structured format\n",
        "    completion = client.beta.chat.completions.parse(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": f\"Today is {today}. Analyze the user message and determine if it contains a calendar event request. Extract relevant details and provide a confidence score between 0 and 1.\"},\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ],\n",
        "        response_format=EventExtraction,  # This tells the API to format output as our model\n",
        "    )\n",
        "    return completion.choices[0].message.parsed\n",
        "\n",
        "# Test with a sample input\n",
        "input_text = \"Let's schedule a 1h team meeting next Tuesday at 2pm with Alice and Bob.\"\n",
        "result = extract_event_info(input_text)\n",
        "result\n"
      ],
      "id": "1AP_kX8SYMT5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“ Step 3: Detail Extraction\n",
        "\n",
        "Now that we've confirmed the text contains a calendar event, we'll extract specific details like the event name, date, duration, and participants.\n",
        "\n",
        "This is the second link in our prompt chain - it takes the description from the previous step and extracts structured information. By breaking this out as a separate step, we give the model a more focused task.\n"
      ],
      "metadata": {
        "id": "916Qmboucppv"
      },
      "id": "916Qmboucppv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB-sFVoiYMT5"
      },
      "outputs": [],
      "source": [
        "# Step 3: Parse event details\n",
        "def parse_event_details(description: str) -> EventDetails:\n",
        "    today = datetime.now().strftime(\"%A, %B %d, %Y\")\n",
        "    completion = client.beta.chat.completions.parse(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": f\"Today is {today}. Extract event details.\"},\n",
        "            {\"role\": \"user\", \"content\": description},\n",
        "        ],\n",
        "        response_format=EventDetails,\n",
        "    )\n",
        "    return completion.choices[0].message.parsed\n",
        "\n",
        "# Use previous output\n",
        "details = parse_event_details(result.description)\n",
        "details"
      ],
      "id": "wB-sFVoiYMT5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“¨ Step 4: Confirmation Generation\n",
        "\n",
        "The final step in our chain is to generate a natural-language confirmation message. This demonstrates how we can convert structured data back into human-friendly text.\n",
        "\n",
        "This approach is powerful because:\n",
        "1. We maintain structured data throughout our workflow (for database storage, API responses, etc.)\n",
        "2. We can still provide a conversational, personalized experience to users\n"
      ],
      "metadata": {
        "id": "13p_XCV1eAHp"
      },
      "id": "13p_XCV1eAHp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh2OM1zNYMT5"
      },
      "outputs": [],
      "source": [
        "# Step 4: Generate confirmation - The third link in our prompt chain\n",
        "def generate_confirmation(event_details: EventDetails) -> EventConfirmation:\n",
        "    completion = client.beta.chat.completions.parse(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful personal assistant named Susie. Generate a friendly, concise confirmation message based on the event details provided. Include all important information in a natural way. Sign off with 'Susie'.\"},\n",
        "            {\"role\": \"user\", \"content\": str(event_details.model_dump())},\n",
        "        ],\n",
        "        response_format=EventConfirmation,\n",
        "    )\n",
        "    return completion.choices[0].message.parsed\n",
        "\n",
        "# Use previous output as input to this function\n",
        "confirmation = generate_confirmation(details)\n",
        "confirmation\n"
      ],
      "id": "yh2OM1zNYMT5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”„ Complete Workflow\n",
        "\n",
        "Now let's connect all three steps into a single workflow. This demonstrates the complete prompt chain:\n",
        "\n",
        "1. **Extract** - Determine if text contains an event\n",
        "2. **Parse** - Extract structured details from the text\n",
        "3. **Generate** - Create a human-friendly confirmation\n",
        "\n",
        "Notice how we include a validation step after the first function call. This is a \"gate\" that prevents low-confidence or non-event inputs from proceeding, saving compute and improving reliability.\n"
      ],
      "metadata": {
        "id": "FBICd5kXeHmp"
      },
      "id": "FBICd5kXeHmp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC5Me1yMYMT5"
      },
      "outputs": [],
      "source": [
        "# Full workflow - The complete prompt chain\n",
        "def process_calendar_request(user_input: str) -> Optional[EventConfirmation]:\n",
        "    # Step 1: Check if input contains a calendar event\n",
        "    extraction = extract_event_info(user_input)\n",
        "\n",
        "    # Validation gate: Only proceed if we're confident this is a calendar event\n",
        "    if not extraction.is_calendar_event or extraction.confidence_score < 0.7:\n",
        "        print(\"Not a calendar event or low confidence. Stopping workflow.\")\n",
        "        return None\n",
        "\n",
        "    # Step 2: Extract detailed information\n",
        "    details = parse_event_details(extraction.description)\n",
        "\n",
        "    # Step 3: Generate user-friendly confirmation\n",
        "    return generate_confirmation(details)\n",
        "\n",
        "# Test the full workflow with our example\n",
        "result = process_calendar_request(input_text)\n",
        "result\n"
      ],
      "id": "fC5Me1yMYMT5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxqZM_GoYMT5"
      },
      "source": [
        "# ðŸ§ª Try Your Own Examples!\n",
        "\n",
        "Now it's your turn to experiment! Try different inputs to see how the system handles various phrasings, edge cases, and non-event texts.\n",
        "\n",
        "### Workshop Challenges:\n",
        "\n",
        "1. **Basic**: Try different ways of phrasing calendar events\n",
        "2. **Intermediate**: Test with ambiguous dates or unusual time formats\n",
        "3. **Advanced**: Try inputs that mix event details with other content\n",
        "4. **Expert**: Modify the models to include additional fields (location, priority, etc.)\n",
        "\n",
        "Remember that the quality of the input prompt greatly affects the output. This is a great opportunity to practice prompt engineering.\n"
      ],
      "id": "PxqZM_GoYMT5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}