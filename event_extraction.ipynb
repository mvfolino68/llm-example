{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvfolino68/llm-example/blob/main/event_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“… LLM Workshop - Event Extraction & Tool Use with AI Systems\n",
        "\n",
        "This notebook demonstrates two powerful capabilities of Large Language Models (specifically OpenAI's gpt-4o-mini):\n",
        "1. Extracting calendar event details from everyday conversational text\n",
        "2. Using function calling to access external tools (like weather data)\n",
        "\n",
        "These practical examples show how LLMs can be integrated into email assistants, chat applications, or productivity tools.\n",
        "\n",
        "## ðŸ‘‹ Goals\n",
        "- Understand prompt chaining for multi-step LLM workflows\n",
        "- Learn how to use Pydantic for structured LLM outputs\n",
        "- Build an event extraction system\n",
        "- Explore how to make LLMs work as reliable components in larger applications\n",
        "- Implement function calling to access external data sources\n"
      ],
      "metadata": {
        "id": "hPx6wQpKY8D1"
      },
      "id": "hPx6wQpKY8D1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Event Extraction with Prompt Chaining\n",
        "\n",
        "## ðŸ” What is Prompt Chaining?\n",
        "Prompt chaining decomposes a complex task into a sequence of simpler steps, where each LLM call processes the output of the previous one. This approach offers several advantages:\n",
        "\n",
        "- Improved accuracy - Each step has a clearer, more focused goal\n",
        "- Better control - You can add validation between steps\n",
        "- Easier debugging - When issues occur, you can identify exactly which step failed\n",
        "\n",
        "In our workflow today:\n",
        "1. First, we determine IF text contains a calendar event\n",
        "2. Then, we extract the detailed event information\n",
        "3. Finally, we generate a natural-language confirmation\n",
        "\n",
        "Let's get started by setting up our environment!\n",
        "\n",
        "[![](https://www.mermaidchart.com/raw/adf23c0b-dba5-4961-b27b-5ea65592e815?theme=light&version=v0.1&format=svg)](\n",
        ")"
      ],
      "metadata": {
        "id": "CyWJG6vAZzS9"
      },
      "id": "CyWJG6vAZzS9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”§ Setup and Installation\n",
        "Add OpenAI secret to Colab Secrets on the left. ðŸ”‘\n",
        "\n",
        "Name the secret OPENAI_API_KEY and make it avilable to the notebook.\n",
        "\n",
        "We'll share a 1password link with openai api key."
      ],
      "metadata": {
        "id": "3W8pkgGzbQLm"
      },
      "id": "3W8pkgGzbQLm"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.66.3"
      ],
      "metadata": {
        "id": "RwtAZZSmAanp"
      },
      "id": "RwtAZZSmAanp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-2AV_D-YMT5"
      },
      "outputs": [],
      "source": [
        "# Setup: Import libraries and initialize client\n",
        "from typing import Optional\n",
        "from datetime import datetime\n",
        "from pydantic import BaseModel, Field\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "\n",
        "# For this you'll need a OpenAI API key.\n",
        "# Enter it in notebook secrets to the left. Name the secret `OPENAI_API_KEY`\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(api_key=api_key)\n",
        "model = \"gpt-4o-mini\"\n",
        "print(\"âœ… Client initialized successfully!\")\n"
      ],
      "id": "H-2AV_D-YMT5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Š Step 1: Data Models with Pydantic\n",
        "A key concept in building reliable AI applications is structured outputs. Instead of parsing free-form text from the LLM, we can have it generate data in precise formats.\n",
        "\n",
        "Pydantic helps us define data models with type validation. When combined with OpenAI's structured output feature, it ensures the LLM generates responses that exactly match our expected schema.\n",
        "\n",
        "Let's define three models for our event extraction workflow:\n"
      ],
      "metadata": {
        "id": "WkkyiJfycMJK"
      },
      "id": "WkkyiJfycMJK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX4XClskYMT5"
      },
      "outputs": [],
      "source": [
        "# Step 1: Define data models\n",
        "class EventExtraction(BaseModel):\n",
        "    description: str            # Cleaned version of the input text\n",
        "    is_calendar_event: bool     # Binary classification: is this an event?\n",
        "    confidence_score: float     # How confident is the model (0.0-1.0)\n",
        "\n",
        "class EventDetails(BaseModel):\n",
        "    name: str                   # Event title/name\n",
        "    date: str                   # ISO 8601 formatted date with time\n",
        "    duration_minutes: int       # How long the event lasts\n",
        "    participants: list[str]     # Who is attending\n",
        "\n",
        "class EventConfirmation(BaseModel):\n",
        "    confirmation_message: str             # Human-friendly confirmation\n",
        "    calendar_link: Optional[str] = None   # Optional calendar link\n",
        "\n",
        "print(\"âœ… Data models defined - these will ensure our LLM outputs follow a consistent format.\")\n"
      ],
      "id": "vX4XClskYMT5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ•µï¸ Step 2: Event Detection\n",
        "The first step in our prompt chain is to determine whether a given text contains a calendar event. This acts as a \"filter\" to avoid wasting compute time on non-event texts.\n",
        "\n",
        "Note how we use the `parse` method with our `EventExtraction` model to get structured output rather than free text. This is a powerful technique introduced in the OpenAI API that ensures data consistency."
      ],
      "metadata": {
        "id": "K5NRQrtjcZHP"
      },
      "id": "K5NRQrtjcZHP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AP_kX8SYMT5"
      },
      "outputs": [],
      "source": [
        "# Step 2: Extract event info - The first link in our prompt chain\n",
        "def extract_event_info(user_input: str) -> EventExtraction:\n",
        "    # Include current date for context (helps with relative dates like \"next Tuesday\")\n",
        "    today = datetime.now().strftime(\"%A, %B %d, %Y\")\n",
        "\n",
        "    # Call the OpenAI API using our structured format\n",
        "    response = client.responses.parse(\n",
        "        model=model,\n",
        "        input=[\n",
        "            {\"role\": \"system\", \"content\": f\"Today is {today}. Analyze the user \\\n",
        "              message and determine if it contains a calendar event request. Extract \\\n",
        "              relevant details and provide a confidence score between 0 and 1.\"},\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ],\n",
        "        text_format=EventExtraction,  # This tells the API to format output as our model\n",
        "    )\n",
        "    return response.output[0].content[0].parsed\n",
        "\n",
        "# Test with a sample input\n",
        "input_text = \"Let's schedule a 1h team meeting next Tuesday at 2pm with Alice and Bob.\"\n",
        "result = extract_event_info(input_text)\n",
        "result\n"
      ],
      "id": "1AP_kX8SYMT5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“ Step 3: Detail Extraction\n",
        "Now that we've confirmed the text contains a calendar event, we'll extract specific details like the event name, date, duration, and participants.\n",
        "\n",
        "This is the second link in our prompt chain - it takes the description from the previous step and extracts structured information. By breaking this out as a separate step, we give the model a more focused task."
      ],
      "metadata": {
        "id": "916Qmboucppv"
      },
      "id": "916Qmboucppv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB-sFVoiYMT5"
      },
      "outputs": [],
      "source": [
        "# Step 3: Parse event details\n",
        "def parse_event_details(description: str) -> EventDetails:\n",
        "    today = datetime.now().strftime(\"%A, %B %d, %Y\")\n",
        "    response = client.responses.parse(\n",
        "        model=model,\n",
        "        input=[\n",
        "            {\"role\": \"system\", \"content\": f\"Today is {today}. Extract event details.\"},\n",
        "            {\"role\": \"user\", \"content\": description},\n",
        "        ],\n",
        "        text_format=EventDetails,\n",
        "    )\n",
        "    return response.output[0].content[0].parsed\n",
        "\n",
        "# Use previous output\n",
        "details = parse_event_details(result.description)\n",
        "details"
      ],
      "id": "wB-sFVoiYMT5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“¨ Step 4: Confirmation Generation\n",
        "The final step in our chain is to generate a natural-language confirmation message. This demonstrates how we can convert structured data back into human-friendly text.\n",
        "\n",
        "This approach is powerful because:\n",
        "1. We maintain structured data throughout our workflow (for database storage, API responses, etc.)\n",
        "2. We can still provide a conversational, personalized experience to users\n"
      ],
      "metadata": {
        "id": "13p_XCV1eAHp"
      },
      "id": "13p_XCV1eAHp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh2OM1zNYMT5"
      },
      "outputs": [],
      "source": [
        "# Step 4: Generate confirmation - The third link in our prompt chain\n",
        "def generate_confirmation(event_details: EventDetails) -> EventConfirmation:\n",
        "    response = client.responses.parse(\n",
        "        model=model,\n",
        "        input=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful personal assistant \\\n",
        "              named Ro. Generate a friendly, concise confirmation message based \\\n",
        "              on the event details provided. Include all important information in a \\\n",
        "              natural way. Sign off with 'Ro'.\"},\n",
        "            {\"role\": \"user\", \"content\": str(event_details.model_dump())},\n",
        "        ],\n",
        "        text_format=EventConfirmation,\n",
        "    )\n",
        "    return response.output[0].content[0].parsed\n",
        "\n",
        "# Use previous output as input to this function\n",
        "confirmation = generate_confirmation(details)\n",
        "print(confirmation.confirmation_message)\n"
      ],
      "id": "yh2OM1zNYMT5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”„ Complete Workflow\n",
        "Now let's connect all three steps into a single workflow. This demonstrates the complete prompt chain:\n",
        "\n",
        "1. Extract - Determine if text contains an event\n",
        "2. Parse - Extract structured details from the text\n",
        "3. Generate - Create a human-friendly confirmation\n",
        "\n",
        "Notice how we include a validation step after the first function call. This is a \"gate\" that prevents low-confidence or non-event inputs from proceeding, saving compute and improving reliability.\n"
      ],
      "metadata": {
        "id": "FBICd5kXeHmp"
      },
      "id": "FBICd5kXeHmp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC5Me1yMYMT5"
      },
      "outputs": [],
      "source": [
        "# Full workflow - The complete prompt chain\n",
        "def process_calendar_request(user_input: str) -> Optional[EventConfirmation]:\n",
        "    # Step 1: Check if input contains a calendar event\n",
        "    extraction = extract_event_info(user_input)\n",
        "\n",
        "    # Validation gate: Only proceed if we're confident this is a calendar event\n",
        "    if not extraction.is_calendar_event or extraction.confidence_score < 0.7:\n",
        "        print(\"Not a calendar event or low confidence. Stopping workflow.\")\n",
        "        return None\n",
        "\n",
        "    # Step 2: Extract detailed information\n",
        "    details = parse_event_details(extraction.description)\n",
        "\n",
        "    # Step 3: Generate user-friendly confirmation\n",
        "    return generate_confirmation(details)\n",
        "\n",
        "# Test the full workflow with our example\n",
        "input_text = \"Let's schedule a 1h team meeting next Friday at 3 with Mike and Rudo.\"\n",
        "result = process_calendar_request(input_text)\n",
        "print(result.confirmation_message)\n"
      ],
      "id": "fC5Me1yMYMT5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Function Calling - Weather API Example\n",
        "\n",
        "## ðŸ”Œ Function Calling with External Tools\n",
        "Another powerful capability of modern LLMs is their ability to interact with external tools through function calling. This allows the model to:\n",
        "\n",
        "1. Recognize when external data is needed\n",
        "2. Format the appropriate function call\n",
        "3. Incorporate the returned data into its response\n",
        "\n",
        "In this example, we'll demonstrate how to use the OpenAI API to query a weather service."
      ],
      "metadata": {
        "id": "z8CnnbMVGDGb"
      },
      "id": "z8CnnbMVGDGb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Define a function to get weather data\n",
        "def get_weather(latitude, longitude):\n",
        "    \"\"\"\n",
        "    Fetches current weather data for the given coordinates.\n",
        "\n",
        "    Args:\n",
        "        latitude (float): The latitude coordinate\n",
        "        longitude (float): The longitude coordinate\n",
        "\n",
        "    Returns:\n",
        "        float: Current temperature in Celsius\n",
        "    \"\"\"\n",
        "    response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\")\n",
        "    data = response.json()\n",
        "    return data['current']['temperature_2m']\n",
        "\n",
        "# Define the tool/function that the model can use\n",
        "tools = [{\n",
        "    \"type\": \"function\",\n",
        "    \"name\": \"get_weather\",\n",
        "    \"description\": \"Get current temperature for provided coordinates in celsius.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"latitude\": {\"type\": \"number\"},\n",
        "            \"longitude\": {\"type\": \"number\"}\n",
        "        },\n",
        "        \"required\": [\"latitude\", \"longitude\"],\n",
        "        \"additionalProperties\": False\n",
        "    },\n",
        "    \"strict\": True\n",
        "}]\n",
        "\n",
        "# Step 1: User asks about the weather\n",
        "input_messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Buffalo today converted to fahrenheit?\"}]\n",
        "\n",
        "# Step 2: Model recognizes need for external data and calls the function\n",
        "response = client.responses.create(\n",
        "    model=model,\n",
        "    input=input_messages,\n",
        "    tools=tools,\n",
        ")\n",
        "\n",
        "# Print the function call being made\n",
        "tool_call = response.output[0]\n",
        "print(f\"Function called: {tool_call.name}\")\n",
        "print(f\"With arguments: {tool_call.arguments}\")\n",
        "\n",
        "# Step 3: We execute the function with the provided arguments\n",
        "args = json.loads(tool_call.arguments)\n",
        "result = get_weather(args[\"latitude\"], args[\"longitude\"])\n",
        "print(f\"Function returned: {result}Â°C\")\n",
        "\n",
        "# Step 4: We send the function result back to the model\n",
        "input_messages.append(tool_call)  # append model's function call message\n",
        "input_messages.append({\n",
        "    \"type\": \"function_call_output\",\n",
        "    \"call_id\": tool_call.call_id,\n",
        "    \"output\": str(result)\n",
        "})\n",
        "\n",
        "# Step 5: Model incorporates the external data into its response\n",
        "response_2 = client.responses.create(\n",
        "    model=model,\n",
        "    input=input_messages,\n",
        "    tools=tools,\n",
        ")\n",
        "print(\"\\nFinal response:\")\n",
        "print(response_2.output_text)\n",
        "\n",
        "# Try with other cities!\n",
        "# You can modify the user query to ask about weather in different locations:\n",
        "# \"What's the weather in Tokyo right now?\"\n",
        "# \"How warm is it in Sydney today?\"\n",
        "# \"Tell me the temperature in Cairo, Egypt\""
      ],
      "metadata": {
        "id": "y_01-UQjGRSh"
      },
      "id": "y_01-UQjGRSh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxqZM_GoYMT5"
      },
      "source": [
        "# ðŸ§ª Try Your Own Examples!\n",
        "Now it's your turn to experiment with both demos:\n",
        "\n",
        "### For Event Extraction:\n",
        "- Basic: Try different ways of phrasing calendar events\n",
        "- Intermediate: Test with ambiguous dates or unusual time formats\n",
        "- Advanced: Try inputs that mix event details with other content\n",
        "- Expert: Modify the models to include additional fields (location, priority, etc.)\n",
        "\n",
        "### For Function Calling:\n",
        "- Try asking about weather in different cities\n",
        "- Modify the function to return additional weather data (humidity, wind speed)\n",
        "- Create a new tool function that does something else entirely!\n",
        "\n",
        "Remember that the quality of the input prompt greatly affects the output. This is a great opportunity to practice prompt engineering.\n",
        "\n",
        "## Helpful Resources\n",
        "- [OpenAI pip](https://github.com/openai/openai-python)\n",
        "- [OpenAI Responses API Docs](https://platform.openai.com/docs/api-reference/responses)\n",
        "- [OpenAI Function Calling Documentation](https://platform.openai.com/docs/guides/function-calling)\n",
        "\n",
        "### Potential Ideas\n",
        "\n",
        "- Patient Data Analytics\n",
        "- Healthcare Chatbots\n",
        "- Device and Drug Comparative Effectiveness\n",
        "- Medical Imaging Insights\n",
        "- Assisted or Automated Diagnosis & Prescription\n",
        "- Early Diagnosis\n",
        "- Process automation (this could apply in many places)\n",
        "- Patient Safety and quality\n",
        "- Auditing of patient interactions"
      ],
      "id": "PxqZM_GoYMT5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}